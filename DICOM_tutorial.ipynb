{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b5258f-9554-46a0-ae39-b1dd086074e9",
   "metadata": {},
   "source": [
    "# A comprehensive guide on DICOM image preprocessing\n",
    "\n",
    "[DICOM](https://en.wikipedia.org/wiki/DICOM) is an international format to store computer tomography (CT) or magnetic resonance imaging (MRI) data.\n",
    "Each DICOM file consists of a single scan image and scan meta information which may include the institution name, patient record ID, instance number in the series as well as technical parameters of the scan, e.g. pixel spacing and patient orientation. Full description of DICOM tags can be found in [DICOM Standard Browser](https://dicom.innolitics.com/ciods). Because both the scanned image and its metadata are stored in a single file, there is no chance of inadvertently mixing either of them with data from a different acquisition.\n",
    "\n",
    "Many challenges on the international data science competition platform [kaggle](www.kaggle.com) are built upon CT or MRI data. Among the most recent is the [RSNA abdominal trauma detection challenge](https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/). In the following, we will use a CT scan from this competition (train_images/14465/21818/135.dcm) to show how to properly load, clean, scale, and save DICOM data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709817d-34ab-4562-bdc2-218d77fa7149",
   "metadata": {},
   "source": [
    "## Bits Allocated and Bits Stored\n",
    "\n",
    "The first thing to note when loading a DICOM image is a possible discrepancy between BitsAllocated (0028,0100) and BitsStored (0028,0101) DICOM tags. \n",
    "\n",
    "A typical case is BitsAllocated=16 and BitsStored=12. CT scans often have 4096 levels of gray, which corresponds to 12 bits. The pixel value is then stored in the 12 least significant bits and must be either an unsigned integer or a binary 2's complement integer, when the PixelRepresentation (0028,0103) flag is set. For 2's complement integer pixel values, the sign bit is the HighBit (0028,0102). The remaining 4 bits can be used, for example, to store overlay planes.\n",
    "\n",
    "Luckily, the organisers of the RSNA abdominal trauma detection challenge [provided](https://www.kaggle.com/code/huiminglin/rsna-2023-abdomen-dicom-standardization) a function that corrects for the BitsAllocated vs BitsStored difference. This function treats the case of PixelRepresentation=1 by basically replacing the leading (BitsAllocated - BitsStored) bits with 1s when pixel values are negative. This ensures that 2's complement pixel values stored in BitsStored - bit representation are properly converted to BitsAllocated - bit representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96569db-9ee5-46bf-8b6f-8788dd9e35ac",
   "metadata": {},
   "source": [
    "![DICOM image before and after correcting for bit representation](img/bit_shift.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e05117-acca-417f-84a4-5612fd45840e",
   "metadata": {},
   "source": [
    "## Hounsfield units\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4fbdd-753c-4ebc-a1cc-9edc082ed166",
   "metadata": {},
   "source": [
    "CT pixel values can be transformed to Hounsfield units (HU). HU are specific for CT data and are proportional to the linear attenuation coefficient (radiodensity) in the scanned structures. HU have a clinical value since a quantitative change in radiodensity may be used to characterize the pathology phenotype. \n",
    "\n",
    "Note that for a given organ, HU may experience significant patient-to-patient variations. The measured radiodensity also depends on some scan parameters, including KVP (0018,0060) and SliceThickness (0018,0050)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53228a1d-7e0e-42c8-864e-6cbe9f5203e6",
   "metadata": {},
   "source": [
    "To transform CT pixel values to HU, two DICOM tags are required: RescaleSlope (0028,1053) and RescaleIntercept (0028,1052):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226f2ad-8057-441f-91c1-360a507ea55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "\n",
    "scan_path = data_dir + 'train_images/9703/39470/485.dcm' # DICOM image path\n",
    "\n",
    "dcm = pydicom.dcmread(scan_path) # read DICOM image\n",
    "\n",
    "img = dcm.pixel_array\n",
    "\n",
    "img = img*dcm.RescaleSlope + dcm.RescaleIntercept # pixel intensities to Hounsfield units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5f114-802a-4d75-927c-5f3fa337e450",
   "metadata": {},
   "source": [
    "It is always recommended to transform CT pixel values to HU. HU do not only contain information relevant to clinical diagnostics and ultimately to ML tasks such as segmentation and classification, but also help to avoid some normalization problems (e.g. PhotometricInterpretation (0028,0004) issues).\n",
    "\n",
    "When the mapping between pixels values and HU is not linear, the RescaleIntercept (0028,1052) tag will not be present. Instead, the ModalityLUTSequence (0028,3000) tag must be used to compute the non-linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46d316-2e35-467c-876f-6a7cf842ca1e",
   "metadata": {},
   "source": [
    "# Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176fca9-1769-44b1-b55d-f89858187693",
   "metadata": {},
   "source": [
    "HU range from -1000 HU for air to +~2000 HU for dense bones. For clinical diagnostics and ML applications, only a certain \"window\" within this range is typically of interest.\n",
    "\n",
    "Windowing is basically performed by clipping all values beyond the window range. For instance, a typical abdominal CT window is centered at 50 HU and is 400 HU wide. To apply this window, all HU below -150 are set to -150 and all HU above +250 are set to +250.\n",
    "\n",
    "The values within the window range can then be linearly mapped to a (0,1) scale for a better visualization or to satisfy requirements for the neural network input. Note that this way of mapping differs from MinMax scaling which converts pixel intensity values to a (0,1) range for each single image. In MinMax scaling, pixels values are normalized against the brightness range specific to each scan. As a result, HU are transformed differently for each image, which might degrade classification performance.\n",
    "\n",
    "Here is how windowing can be coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6be9f0-38c7-40ef-932e-8be7dbde878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "WindowCenter, WindowWidth = 50, 400 # typical HU window for abdominal scan\n",
    "\n",
    "vmin = WindowCenter - WindowWidth/2 # -150\n",
    "vmax = WindowCenter + WindowWidth/2 # 250\n",
    "        \n",
    "img = np.clip(img, vmin, vmax) # clip\n",
    "        \n",
    "img = (img - vmin)/(vmax - vmin) # stretch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc7bba-36ee-4000-a131-75b614092e83",
   "metadata": {},
   "source": [
    "![CT scann with different windows applied](img/windowing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d91ab-dc01-4b24-a63a-db45c7a5491b",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cff82-8355-4905-80fa-befdeda71a26",
   "metadata": {},
   "source": [
    "CT scans often contain elements irrelevant to diagnostics, s.t. a patient table or an emergency button with cable. These elements impact embeddings and may make predictions less reliable.\n",
    "\n",
    "To remove the annoying stuff, we will first define a mask by choosing all pixels above the bottom edge of the abdominal window (HU=-150). Afterwards, we use the morphology.remove_small_objects function from the scikit-image package to remove all objects smaller than 12000 mm2, which we emprically found to be the minimal body cross-sectional area on the scan.\n",
    "\n",
    "Windowing or clipping pixel intensities at HU=-150 makes lungs appear as holes on the image mask since they are normally filled with air (HU = -1000). We will then apply the skimage.morphology.remove_small_holes function to fill in such areas as well as smaller holes that may appear on the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c659c81-4f6c-4898-8d9f-ac7b224400df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "crop_HU_thr = -150 # bottom edge of the abdominal window\n",
    "min_body_area_mm2 = 12000 # minimal body cross-sectionl area (e.g. legs), defined heuristically\n",
    "max_hole_area_mm2 = 10000 # maximal area of the hole in body mask to fill (e.g. lungs), defined heuristically\n",
    "background_HU = -1000 # HU of air\n",
    "\n",
    "pixel_size_mm2 = np.prod(dcm.PixelSpacing) # actual pixel size on the image\n",
    "\n",
    "keep_mask = img>crop_HU_thr\n",
    "\n",
    "# remove small objects (patient table, arm, etc) \n",
    "mask1 = skimage.morphology.remove_small_objects(keep_mask, min_size=int(min_body_area_mm2/pixel_size_mm2))\n",
    "        \n",
    "# remove holes in the body cross-section (lungs, etc)\n",
    "mask2 = skimage.morphology.remove_small_holes(mask1, area_threshold=int(max_hole_area_mm2/pixel_size_mm2))\n",
    "    \n",
    "img[~keep_mask] = background_HU # erase everything outside the body mask by assigning to background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a2caa-6d08-40eb-8c0f-9a168b159d65",
   "metadata": {},
   "source": [
    "![Removing irrelevant objects with masking](img/masks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5110e6-7d07-473e-831a-49ff88258b5d",
   "metadata": {},
   "source": [
    "Note that this approach implies that windowing is applied after cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3355e-c92f-4056-ba58-b0b0948b4f3b",
   "metadata": {},
   "source": [
    "![Applying windowing after cleaning](img/clean_windowed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac413300-7769-42cc-80d6-45ad66db35b9",
   "metadata": {},
   "source": [
    "# Cropping and Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fd016-4fab-4e36-8a2f-64e7d3dd44f9",
   "metadata": {},
   "source": [
    "The image often has a lot of black space after cleaning. It often makes sense to crop out the relevant part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff89b1a-d183-4aee-bf36-24022d6c0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "x, y, w, h = cv2.boundingRect((mask2*255).astype('uint8')) # obtain rectangular area around the mask\n",
    "\n",
    "img = img[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e332734-332a-4928-b651-30021f1a5edf",
   "metadata": {},
   "source": [
    "When the resulting image must have fixed dimensions (for example, for a CNN input), a simple resizing operation can be applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567e737-0725-4c93-b19e-8a93b6e89b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = (256, 256) # required output dimensions, px\n",
    "\n",
    "img = cv2.resize(img, dsize=output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fc582-2aac-4d4c-93bc-b9c184332e30",
   "metadata": {},
   "source": [
    "The size of a single pixel (pixel spacing) can vary from one patient to another. When the data amount is small, it can be beneficial to keep the pixel spacing fixed on the output image, say 1.5mm x 1.5mm. In this case, the resizing operation gets a bit more complicated: we must first scale the pixel dimensions, then place the initial image onto the background of desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b144793-33de-41e2-b826-3b06af611194",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_spacing = (1.5, 1.5) # required output spacing, mm\n",
    "height_new, width_new = (256, 256) # required output size, px\n",
    "\n",
    "img = cv2.resize(img, dsize=None, fx = dcm.PixelSpacing[1]/pixel_spacing[1], fy=dcm.PixelSpacing[0]/pixel_spacing[0]) # scale pixels\n",
    "        \n",
    "height, width = img.shape # actual dimensions\n",
    "\n",
    "img = img[max((height-height_new)//2,0):(height+height_new)//2,\n",
    "        max((width-width_new)//2,0):(width+width_new)//2] # use the central part of the image if actual dimensions are larger than required dimensions\n",
    "        \n",
    "height, width = img.shape # recompute dimensions after cropping the central part\n",
    "\n",
    "# position of the old image inside the new image\n",
    "x0 = width_new//2 - width//2 \n",
    "y0 = height_new//2 - height//2\n",
    "                \n",
    "img_new = np.ones((height_new, width_new))*img.min() # position of the old image inside the new image\n",
    "        \n",
    "img_new[y0:y0 + height, x0:x0 + width] = img # place the old image in the center of the new image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833153f4-f18f-485f-bafb-12d5470705fe",
   "metadata": {},
   "source": [
    "![Fixed dimensions vs Fixed spacing approach](img/resizing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea39df1-d080-4374-b2b0-20ed250db3d3",
   "metadata": {},
   "source": [
    "# Conversion to PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700a017-9566-4fbb-9973-08fb9000e886",
   "metadata": {},
   "source": [
    "DICOM images are often furnished without any compression. In addition, they usually contain HU units on the full scale, before windowing. As a result, storing a DICOM image needs a lot of space while loading a DICOM image requires a lot of time. To train neural networks faster and save storage space, DICOM images can be converted to PNG files.\n",
    "\n",
    "To ensure optimal compression, we choose the minimal png depth that is sufficient to encode N=WindowWidth distinct HU values. We then map the pixel values from [0,1] back to the [0,WindowWidth] scale and save as a PNG image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88efe7d-3666-4c90-bf02-8c8766e5e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import png\n",
    "\n",
    "WindowCenter, WindowWidth = 50,400\n",
    "\n",
    "bitdepth = np.ceil(np.log2(WindowWidth)).astype(int)\n",
    "\n",
    "with open('img.png', 'wb') as f:\n",
    "    png_writer = png.Writer(img.shape[1], img.shape[0], bitdepth=bitdepth)  \n",
    "    png_writer.write(f, (img*WindowWidth).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9477b-e7df-4dc0-ba6c-c8135ab80e70",
   "metadata": {},
   "source": [
    "This operation reduced the initial image file from 242Kb to just 41Kb, which is around 6x.\n",
    "\n",
    "For convinience, I created a [dicomprocessing module](dicomprocessing/) that performs all the described operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
